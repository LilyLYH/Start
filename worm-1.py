#!/usr/bin/env python3
"""
ç°ä»£åŒ–Pythonçˆ¬è™«ç¤ºä¾‹ - ä½¿ç”¨å¼‚æ­¥å’Œç±»å‹æ³¨è§£
ç›®æ ‡ï¼šçˆ¬å–GitHub Trendingé¡µé¢ä¿¡æ¯
Python 3.14+ ç‰¹æ€§æ¼”ç¤º
"""

import asyncio
import json
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional, Any
from enum import Enum

import aiohttp
from bs4 import BeautifulSoup
import httpx  # æ›¿ä»£requestsçš„ç°ä»£åŒ–HTTPå®¢æˆ·ç«¯

# Python 3.14+ ç‰¹æ€§ï¼šç±»å‹åˆ«åæ›´æ¸…æ™°çš„è¯­æ³•
type RepoURL = str
type StarCount = int

# ä½¿ç”¨dataclasså®šä¹‰æ•°æ®ç»“æ„ï¼ˆPython 3.7+ï¼‰
@dataclass
class GitHubRepo:
    """GitHubä»“åº“æ•°æ®ç»“æ„"""
    name: str
    url: RepoURL
    description: Optional[str]
    stars: StarCount
    forks: int
    language: Optional[str]
    today_stars: Optional[int] = None
    
    # Python 3.14+ å®éªŒæ€§ç‰¹æ€§ï¼šæ¨¡å¼åŒ¹é…çš„å¦ä¸€ç§ç”¨æ³•
    def is_popular(self) -> bool:
        """åˆ¤æ–­ä»“åº“æ˜¯å¦å—æ¬¢è¿"""
        match self:
            case GitHubRepo(stars=s) if s > 10000:
                return True
            case GitHubRepo(today_stars=ts) if ts and ts > 500:
                return True
            case _:
                return False

class SortBy(Enum):
    """æ’åºæ–¹å¼æšä¸¾"""
    STARS = "stars"
    FORKS = "forks"
    TODAY = "today"

class GitHubTrendingScraper:
    """GitHub Trendingçˆ¬è™«ç±»"""
    
    BASE_URL = "https://github.com/trending"
    
    def __init__(self, language: str = "", since: str = "daily"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            language: ç¼–ç¨‹è¯­è¨€è¿‡æ»¤ (å¦‚ "python", "javascript")
            since: æ—¶é—´èŒƒå›´ ("daily", "weekly", "monthly")
        """
        self.language = language
        self.since = since
        self.session: Optional[httpx.AsyncClient] = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # ä½¿ç”¨ç°ä»£åŒ–HTTPå®¢æˆ·ç«¯ï¼ˆæ”¯æŒHTTP/2ï¼‰
        self.session = httpx.AsyncClient(
            timeout=30.0,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "application/vnd.github.v3+json"
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.session:
            await self.session.aclose()
    
    def _build_url(self) -> str:
        """æ„å»ºè¯·æ±‚URL"""
        url = self.BASE_URL
        if self.language:
            url += f"/{self.language}"
        url += f"?since={self.since}"
        return url
    
    async def fetch_page(self) -> str:
        """å¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
        if not self.session:
            raise RuntimeError("Session not initialized")
        
        url = self._build_url()
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        try:
            response = await self.session.get(url)
            response.raise_for_status()
            return response.text
        except httpx.HTTPError as e:
            print(f"HTTPé”™è¯¯: {e}")
            return ""
    
    def parse_repos(self, html: str) -> List[GitHubRepo]:
        """è§£æHTMLï¼Œæå–ä»“åº“ä¿¡æ¯"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, "html.parser")
        repos = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ä»“åº“é¡¹ç›®
        repo_elements = soup.find_all("article", class_="Box-row")
        
        for repo_element in repo_elements:
            try:
                # æå–ä»“åº“åç§°å’ŒURL
                title_element = repo_element.find("h2", class_="h3")
                if not title_element:
                    continue
                    
                link = title_element.find("a")
                if not link:
                    continue
                    
                repo_name = link.get_text(strip=True).replace(" ", "")
                repo_url = f"https://github.com{link['href']}"
                
                # æå–æè¿°
                desc_element = repo_element.find("p", class_="col-9")
                description = desc_element.get_text(strip=True) if desc_element else None
                
                # æå–ç¼–ç¨‹è¯­è¨€
                lang_element = repo_element.find("span", itemprop="programmingLanguage")
                language = lang_element.get_text(strip=True) if lang_element else None
                
                # æå–æ˜Ÿæ ‡å’Œforkæ•°é‡ï¼ˆä½¿ç”¨æ›´å¥å£®çš„é€‰æ‹©å™¨ï¼‰
                stars_elements = repo_element.find_all("a", class_="Link--muted")
                stars = 0
                forks = 0
                today_stars = None
                
                for elem in stars_elements:
                    text = elem.get_text(strip=True)
                    if "stars" in elem.get("href", ""):
                        # è§£ææ˜Ÿæ ‡æ•°ï¼ˆå¯èƒ½åŒ…å«åƒä½åˆ†éš”ç¬¦ï¼‰
                        stars_text = text.replace(",", "").replace("k", "000")
                        if "stars today" in text:
                            # æå–ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡
                            today_match = text.split()
                            if today_match:
                                today_stars_text = today_match[0].replace(",", "")
                                today_stars = int(today_stars_text) if today_stars_text.isdigit() else None
                        else:
                            stars = int(''.join(filter(str.isdigit, stars_text)))
                    elif "fork" in elem.get("href", ""):
                        forks_text = text.replace(",", "")
                        forks = int(''.join(filter(str.isdigit, forks_text))) if forks_text else 0
                
                # åˆ›å»ºä»“åº“å¯¹è±¡
                repo = GitHubRepo(
                    name=repo_name,
                    url=repo_url,
                    description=description,
                    stars=stars,
                    forks=forks,
                    language=language,
                    today_stars=today_stars
                )
                
                repos.append(repo)
                
            except Exception as e:
                print(f"è§£æä»“åº“æ—¶å‡ºé”™: {e}")
                continue
        
        return repos
    
    def sort_repos(self, repos: List[GitHubRepo], by: SortBy = SortBy.STARS) -> List[GitHubRepo]:
        """æ’åºä»“åº“åˆ—è¡¨"""
        match by:
            case SortBy.STARS:
                return sorted(repos, key=lambda x: x.stars, reverse=True)
            case SortBy.FORKS:
                return sorted(repos, key=lambda x: x.forks, reverse=True)
            case SortBy.TODAY:
                # è¿‡æ»¤å‡ºæœ‰ä»Šæ—¥æ˜Ÿæ ‡æ•°æ®çš„ä»“åº“
                filtered = [r for r in repos if r.today_stars is not None]
                return sorted(filtered, key=lambda x: x.today_stars or 0, reverse=True)
            case _:
                return repos
    
    def save_to_json(self, repos: List[GitHubRepo], filename: str = "github_trending.json"):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        # å°†dataclasså¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        repos_dict = [repo.__dict__ for repo in repos]
        
        data = {
            "metadata": {
                "language": self.language or "all",
                "since": self.since,
                "fetched_at": datetime.now().isoformat(),
                "total_repos": len(repos)
            },
            "repositories": repos_dict
        }
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    async def scrape(self, sort_by: SortBy = SortBy.STARS) -> List[GitHubRepo]:
        """ä¸»çˆ¬å–æ–¹æ³•"""
        html = await self.fetch_page()
        repos = self.parse_repos(html)
        sorted_repos = self.sort_repos(repos, sort_by)
        
        # è¾“å‡ºæ‘˜è¦ä¿¡æ¯
        print(f"\n{'='*50}")
        print(f"GitHub Trending åˆ†æç»“æœ ({self.language or 'æ‰€æœ‰è¯­è¨€'})")
        print(f"{'='*50}")
        
        for i, repo in enumerate(sorted_repos[:10], 1):
            popular_flag = "ğŸ”¥" if repo.is_popular() else "  "
            today_stars_text = f"(ä»Šæ—¥+{repo.today_stars})" if repo.today_stars else ""
            print(f"{i:2d}. {popular_flag} {repo.name:40} â­ {repo.stars:>6} {today_stars_text:10}")
            if repo.description:
                print(f"    {repo.description[:80]}...")
            if repo.language:
                print(f"    ğŸ·ï¸  {repo.language}")
            print()
        
        return sorted_repos

# Python 3.14+ ç‰¹æ€§ï¼šä½¿ç”¨asyncio.run()è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("GitHub Trending çˆ¬è™«å¯åŠ¨...\n")
    
    # ç¤ºä¾‹1ï¼šçˆ¬å–Pythonè¶‹åŠ¿ä»“åº“
    async with GitHubTrendingScraper(language="python", since="daily") as scraper:
        python_repos = await scraper.scrape(sort_by=SortBy.TODAY)
        scraper.save_to_json(python_repos, "github_trending_python.json")
    
    # ç¤ºä¾‹2ï¼šçˆ¬å–æ‰€æœ‰è¯­è¨€è¶‹åŠ¿ï¼ˆå¼‚æ­¥å¹¶å‘ç¤ºä¾‹ï¼‰
    print("\n" + "="*50)
    print("åŒæ—¶çˆ¬å–å¤šç§è¯­è¨€è¶‹åŠ¿...")
    
    languages = ["python", "javascript", "go", "rust"]
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for lang in languages:
            scraper = GitHubTrendingScraper(language=lang, since="weekly")
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…éœ€è¦é€‚é…aiohttp
            tasks.append(asyncio.create_task(scraper.scrape()))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for lang, repos in zip(languages, results):
            if isinstance(repos, Exception):
                print(f"{lang}: çˆ¬å–å¤±è´¥ - {repos}")
            else:
                print(f"{lang}: çˆ¬å–äº† {len(repos)} ä¸ªä»“åº“")
    
    print("\nçˆ¬å–å®Œæˆï¼")

# Python 3.14+ ç‰¹æ€§ï¼šæ›´ç®€æ´çš„ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    asyncio.run(main())